{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Dynamic Programming </h1></center>\n",
    "<hr>\n",
    "\n",
    "### Simple GridWorld implementation of Policy Evaluation, Policy Iteration and Value Iteration algorithms \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    grid = np.zeros((4,4))  # Initialise simple Gridworld\n",
    "    reward = -1             # Reward at each step\n",
    "    d_values = [-1,1,-1,1]  # ['up','down','left','right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones((4,4,4))*0.25  # Random policy\n",
    "env = Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_evaluation(env, pi):\n",
    "    \"\"\"\n",
    "    Policy Evaluation algorithm\n",
    "    \n",
    "    Parameters passed - Environment object and Policy\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = 1.0e-10  # Smaller the theta value, more precise the result\n",
    "    gamma = 1        # Discount factor\n",
    "    v = 0\n",
    "    delta = 1        # Algorithm parameter\n",
    "    \n",
    "    while delta > theta:\n",
    "        \n",
    "        delta = 0\n",
    "        grid_old = env.grid.copy()\n",
    "        \n",
    "        for row in range(0,4):\n",
    "            for col in range(0,4):  # Iterate through all states\n",
    "                \n",
    "                if ((row,col) == (0,0)) or ((row,col) == (3,3)):  # Start state and terminal state\n",
    "                    continue\n",
    "                    \n",
    "                v = env.grid[row,col]\n",
    "                env.grid[row,col] = 0   # Later gets updated\n",
    "                \n",
    "                for next_a in range(0,2):  # Next action - up or down\n",
    "                    \n",
    "                    if row == 0 and next_a == 0:\n",
    "                        env.grid[row,col] += pi[row,col,next_a]*(env.reward + gamma*grid_old[row,col])\n",
    "                    elif row == 3 and next_a == 1:\n",
    "                        env.grid[row,col] += pi[row,col,next_a]*(env.reward + gamma*grid_old[row,col])\n",
    "                    else:\n",
    "                        env.grid[row,col] += pi[row,col,next_a]*(env.reward + gamma*grid_old[row+env.d_values[next_a],col])\n",
    "                 \n",
    "                for next_a in range(2,4):  # Next action - left or right\n",
    "                    \n",
    "                    if col == 0 and next_a == 2:\n",
    "                        env.grid[row,col] += pi[row,col,next_a]*(env.reward + gamma*grid_old[row,col])\n",
    "                    elif col == 3 and next_a == 3:\n",
    "                        env.grid[row,col] += pi[row,col,next_a]*(env.reward + gamma*grid_old[row,col])\n",
    "                    else:\n",
    "                        env.grid[row,col] += pi[row,col,next_a]*(env.reward + gamma*grid_old[row,col+env.d_values[next_a]])\n",
    "                \n",
    "                delta = max(delta, abs(v - env.grid[row,col]))\n",
    "    \n",
    "    return env.grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid value function: \n",
      "\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print \"Grid value function: \\n\\n\", pol_evaluation(env, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_iteration(env, pi):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm\n",
    "    \n",
    "    Parameters passed - Environment object and Policy\n",
    "    \"\"\"\n",
    "    \n",
    "    gamma = 1\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        stable = True\n",
    "        \n",
    "        for row in range(0,4):\n",
    "            for col in range(0,4):  # Iterate through all states\n",
    "                    \n",
    "                old_action = np.zeros(4)\n",
    "                for j in range(0,4):\n",
    "                    old_action[j] = pi[row,col,j] # Store current policy in `old_action`\n",
    "                                                  # Policy gets updated later in the loop\n",
    "                \n",
    "                value = np.zeros(4)  # Array for action-values\n",
    "                \n",
    "                for next_a in range(0,2):  # Next action - up or down\n",
    "                    \n",
    "                    if row == 0 and next_a == 0:\n",
    "                        value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    elif row == 3 and next_a == 1:\n",
    "                        value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    else:\n",
    "                        value[next_a] = env.reward + gamma*env.grid[row+env.d_values[next_a],col]\n",
    "                 \n",
    "                for next_a in range(2,4):  # Next action - left or right\n",
    "                    \n",
    "                    if col == 0 and next_a == 2:\n",
    "                        value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    elif col == 3 and next_a == 3:\n",
    "                        value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    else:\n",
    "                        value[next_a] = env.reward + gamma*env.grid[row,col+env.d_values[next_a]]\n",
    "                \n",
    "                max_arg = np.max(value)  # Returns highest value of action\n",
    "                pi[row,col,:] = 0        # Gets updated further\n",
    "                \n",
    "                for i in range(0,4):\n",
    "                    if value[i] == max_arg:  # Returns index of best action\n",
    "                        pi[row,col,i] += 1   # Sets policy value for best action as 1\n",
    "                \n",
    "                prob_d = np.sum(pi[row,col,:], axis=0)  # Checks for sum of policy values\n",
    "                \n",
    "                for i in range(0,4):\n",
    "                    if pi[row,col,i] != 0:          # Possible that more than one action has same value\n",
    "                        pi[row,col,i] = 1/prob_d    # Hence calculates probability of action taken\n",
    "                         \n",
    "\n",
    "                for j in range(0,4):\n",
    "                    if(old_action[j] != pi[row,col,j]):  # Checks if old policy matches new policy\n",
    "                        stable = False\n",
    "                        break\n",
    "                        \n",
    "        if stable == False:\n",
    "            env.grid = pol_evaluation(env,pi)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return env.grid, pi               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid value function: \n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      " Improved Policy: \n",
      "\n",
      "[[[0.5  0.   0.5  0.  ]\n",
      "  [0.   0.   1.   0.  ]\n",
      "  [0.   0.   1.   0.  ]\n",
      "  [0.   0.5  0.5  0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [0.5  0.   0.5  0.  ]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.   1.   0.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.   0.5  0.   0.5 ]\n",
      "  [0.   1.   0.   0.  ]]\n",
      "\n",
      " [[0.5  0.   0.   0.5 ]\n",
      "  [0.   0.   0.   1.  ]\n",
      "  [0.   0.   0.   1.  ]\n",
      "  [0.   0.5  0.   0.5 ]]]\n"
     ]
    }
   ],
   "source": [
    "grid, pol = pol_iteration(env,pi)\n",
    "\n",
    "print \"Grid value function: \\n\\n\", grid\n",
    "print \"\\n\\n Improved Policy: \\n\\n\", pol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_iteration(env, pi):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm\n",
    "    \n",
    "    Parameters passed - Environment object and Policy\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = 1.0e-10  # Smaller the theta value, more precise the result\n",
    "    gamma = 1        # Discount factor\n",
    "    v = 0\n",
    "    delta = 1        # Algorithm parameter\n",
    "    \n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for row in range(0,4):\n",
    "            for col in range(0,4):  # Iterate through all states\n",
    "                \n",
    "                if ((row,col) == (0,0)) or ((row,col) == (3,3)):  # Start state and terminal state\n",
    "                    continue\n",
    "                \n",
    "                v = env.grid[row,col]\n",
    "                grid = np.zeros(4)  # Array of all action-values\n",
    "                \n",
    "                for next_a in range(0,2):  # Next action - up or down\n",
    "                    \n",
    "                    if row == 0 and next_a == 0:\n",
    "                        grid[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    elif row == 3 and next_a == 1:\n",
    "                        grid[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    else:\n",
    "                        grid[next_a] = env.reward + gamma*env.grid[row+env.d_values[next_a],col]\n",
    "                 \n",
    "                for next_a in range(2,4):  # Next action - left or right\n",
    "                    \n",
    "                    if col == 0 and next_a == 2:\n",
    "                        grid[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    elif col == 3 and next_a == 3:\n",
    "                        grid[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                    else:\n",
    "                        grid[next_a] = env.reward + gamma*env.grid[row,col+env.d_values[next_a]]\n",
    "                        \n",
    "                env.grid[row,col] = np.max(grid) # Assigning highest action-value to current state\n",
    "                \n",
    "                delta = max(delta, abs(v - env.grid[row,col]))\n",
    "                \n",
    "    \"\"\"\n",
    "    Algorithm to find optimal policy\n",
    "    \"\"\"\n",
    "    \n",
    "    for row in range(0,4):\n",
    "        for col in range(0,4):  # Iterate through all states\n",
    "                                  \n",
    "            value = np.zeros(4)  # Array for action-values\n",
    "                \n",
    "            for next_a in range(0,2):  # Next action - up or down\n",
    "                    \n",
    "                if row == 0 and next_a == 0:\n",
    "                    value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                elif row == 3 and next_a == 1:\n",
    "                    value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                else:\n",
    "                    value[next_a] = env.reward + gamma*env.grid[row+env.d_values[next_a],col]\n",
    "                 \n",
    "            for next_a in range(2,4):  # Next action - left or right\n",
    "                    \n",
    "                if col == 0 and next_a == 2:\n",
    "                    value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                elif col == 3 and next_a == 3:\n",
    "                    value[next_a] = env.reward + gamma*env.grid[row,col]\n",
    "                else:\n",
    "                    value[next_a] = env.reward + gamma*env.grid[row,col+env.d_values[next_a]]\n",
    "                \n",
    "            max_arg = np.argmax(value)  # Returns index of best action\n",
    "            pi[row,col,:] = 0           \n",
    "                \n",
    "            pi[row,col,max_arg] = 1     # Deterministic policy to take best possible action                   \n",
    "                \n",
    "    return env.grid, pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final grid value function: \n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      " Optimal Policy: \n",
      "\n",
      "[[[1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "grid, policy = val_iteration(env,pi)\n",
    "\n",
    "print \"Final grid value function: \\n\\n\", grid\n",
    "print \"\\n\\n Optimal Policy: \\n\\n\", policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
