{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GridWorld Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "\n",
    "# setting the environment using the GridWorld Class\n",
    "class GridWorld:\n",
    "    \n",
    "    \"\"\"\n",
    "    Grid environment with following stochastic property:\n",
    "    | Agent Action | Possible Actions  |  Probability  |\n",
    "    | :----------: | :---------------: | :-----------: |\n",
    "    |      UP      |  UP, RIGHT, LEFT  | 0.8, 0.1, 0.1 |\n",
    "    |     DOWN     | DOWN, RIGHT, LEFT | 0.8, 0.1, 0.1 |\n",
    "    |     LEFT     |  LEFT, UP, DOWN   | 0.8, 0.1, 0.1 |\n",
    "    |    RIGHT     |  RIGHT, UP, DOWN  | 0.8, 0.1, 0.1 |\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # listing out all the possible actions...\n",
    "    POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']\n",
    "\n",
    "    \n",
    "    def __init__(self, size, rewards, actions):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the GridWorld object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size : tuple (row,col)\n",
    "        \n",
    "        rewards: dict {(row,col):int}\n",
    "            A dictionary with reward values for each state in the grid\n",
    "            \n",
    "        actions: dict ({row,col}:list)\n",
    "            A dictionary that associates all possible actions for each state\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.height, self.width = size\n",
    "\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "        self.num_states = np.prod(size)\n",
    "        self.num_actions = len(GridWorld.POSSIBLE_ACTIONS)\n",
    "\n",
    "        \n",
    "    \n",
    "    def _limit_coordinates(self, state):\n",
    "        \n",
    "        \"\"\"\n",
    "        Limits the coordinates if/after collision with grid wall\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple (row, col)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state: tuple(row, col)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # pretty straightforward...\n",
    "        i, j = state\n",
    "        \n",
    "        if i < 0:\n",
    "            i = 0\n",
    "            \n",
    "        elif i > self.height - 1:\n",
    "            i = self.height - 1\n",
    "            \n",
    "        if j < 0:\n",
    "            j = 0\n",
    "            \n",
    "        elif j > self.width - 1:\n",
    "            j = self.width - 1\n",
    "            \n",
    "        return (i, j)\n",
    "\n",
    "    \n",
    "    def _new_state_reward(self, action, state):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the coordinates of a resultant state and its rewards\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action: char\n",
    "            The character representing the action taken\n",
    "            \n",
    "        state: tuple (row, col)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state: tuple (row, col)\n",
    "            The new state reached by taking the action\n",
    "            \n",
    "        reward: int\n",
    "            The reward for the state\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # again, we are updating the agent's location...\n",
    "        i, j = state\n",
    "        \n",
    "        if action == 'U':\n",
    "            i, j = i - 1, j\n",
    "            \n",
    "        elif action == 'D':\n",
    "            i, j = i + 1, j\n",
    "            \n",
    "        elif action == 'R':\n",
    "            i, j = i, j + 1\n",
    "            \n",
    "        elif action == 'L':\n",
    "            i, j = i, j - 1\n",
    "\n",
    "        # make sure the new state is not out of grid\n",
    "        new_state = self._limit_coordinates((i, j))\n",
    "\n",
    "        \n",
    "        return new_state, self.rewards.get(new_state)\n",
    "    \n",
    "\n",
    "    def transition(self, action, state, choose=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        The stochastic transition model of the grid\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : char\n",
    "            The character representing the action taken\n",
    "            \n",
    "        state : tuple (row, col)\n",
    "            The current state from where transition is occuring\n",
    "            \n",
    "        choose : boolean\n",
    "            If True: environment takes stochastic action and returns resultant\n",
    "                     state and reward\n",
    "                     \n",
    "            If False: environment returns a list of all possible actions with\n",
    "                      corresponding reward and probabilties\n",
    "                      \n",
    "        Returns\n",
    "        -------\n",
    "        If choose == True\n",
    "        \n",
    "            state : tuple (row, col)\n",
    "            \n",
    "            reward: int\n",
    "            \n",
    "        If choose == Flase\n",
    "            A list with following tuple:\n",
    "            \n",
    "                prob: float\n",
    "                    Probabilty with which environment selects the transition\n",
    "                    \n",
    "                state: tuple (row, col)\n",
    "                \n",
    "                reward: int\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # as the blueprint above suggests, we create the function body...\n",
    "        def stochastic_transition(possible_actions, prob):\n",
    "            \n",
    "            if not choose:\n",
    "                # create and return a list of all possible actions\n",
    "                result = []\n",
    "                \n",
    "                for i, a in enumerate(possible_actions):\n",
    "                    coord, reward = self._new_state_reward(a, state)\n",
    "                    result.append((prob[i], coord, reward))\n",
    "                    \n",
    "                return result\n",
    "            \n",
    "            else:\n",
    "                # choose a random action with given probabilities\n",
    "                a = np.random.choice(possible_actions, 1, p = prob)\n",
    "                coord, reward = self._new_state_reward(a, state)\n",
    "                \n",
    "                return coord, reward\n",
    "\n",
    "        \n",
    "        # we already have the state transition probabilities, they are initialised in the next function\n",
    "        # going up the grif\n",
    "        if action == 'U':\n",
    "            return stochastic_transition(['U', 'R', 'L'], [0.8, 0.1, 0.1])\n",
    "        \n",
    "        # going down the grid\n",
    "        elif action == 'D':\n",
    "            return stochastic_transition(['D', 'R', 'L'], [0.8, 0.1, 0.1])\n",
    "        \n",
    "        # going right\n",
    "        elif action == 'R':\n",
    "            return stochastic_transition(['R', 'U', 'D'], [0.8, 0.1, 0.1])\n",
    "        \n",
    "        # going left\n",
    "        elif action == 'L':\n",
    "            return stochastic_transition(['L', 'U', 'D'], [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "        \n",
    "def grid():\n",
    "    \"\"\"Utility function, returns 4x4 GridWorld object with rewards and actions\n",
    "    \"\"\"\n",
    "\n",
    "    # dict with rewards for states of the grid\n",
    "    rewards = {\n",
    "        (0, 0): -1, (0, 1): -1, (0, 2): -1, (0, 3): -1,\n",
    "        (1, 0): -1, (1, 1): -1, (1, 2): -1, (1, 3): -1,  # start state is 1x0\n",
    "        (2, 0): -1, (2, 1): -70, (2, 2): -1, (2, 3): -1,  # bad state is 2x1\n",
    "        (3, 0): -1, (3, 1): -1, (3, 2): -1, (3, 3): 100  # goal state is 3x3\n",
    "    }\n",
    "\n",
    "    # dict with actions allowed for the grid states\n",
    "    actions = {\n",
    "        (0, 0): ['R', 'D'], (0, 1): ['R', 'L', 'D'],\n",
    "        (0, 2): ['R', 'L', 'D'], (0, 3): ['L', 'D'],\n",
    "        (1, 0): ['R', 'U', 'D'], (1, 1): ['R', 'L', 'U', 'D'],\n",
    "        (1, 2): ['R', 'L', 'U', 'D'], (1, 3): ['L', 'U', 'D'],\n",
    "        (2, 0): ['R', 'U', 'D'], (2, 1): ['R', 'L', 'U', 'D'],\n",
    "        (2, 2): ['R', 'L', 'U', 'D'], (2, 3): ['L', 'U', 'D'],\n",
    "        (3, 0): ['R', 'U'], (3, 1): ['R', 'L', 'U'],\n",
    "        (3, 2): ['R', 'L', 'U'], (3, 3): []\n",
    "    }\n",
    "\n",
    "    return GridWorld(size=(4, 4), rewards=rewards, actions=actions)\n",
    "\n",
    "\n",
    "def print_grid(env, content_dict):\n",
    "    \n",
    "    \"\"\" \n",
    "    Utility function that prints the grid environment with given content\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    env: GridWorld\n",
    "    \n",
    "    content_dict: dict {(row,col):object}\n",
    "    \n",
    "    \"\"\"\n",
    "    # getting even coordinates in the grid..\n",
    "    grid = np.arange(env.num_states, dtype=object).reshape(env.height, env.width)\n",
    "    \n",
    "    # content_dict maps the coordinate to the content(policy, value function, etc)...\n",
    "    for coord, content in content_dict.items():\n",
    "        grid[coord[0], coord[1]] = content\n",
    "    \n",
    "    # printing out the np matrix\n",
    "    print(grid)\n",
    "\n",
    "\n",
    "def play_game(env, start, end, policy):\n",
    "    \n",
    "    \"\"\"\n",
    "    Utility function that follows given policy from start to end and\n",
    "    returns a list of action, state and reward for each step\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "    \n",
    "    start : (row, col)\n",
    "    \n",
    "    end : (row, col)\n",
    "    \n",
    "    policy : list of optimal actions for each state\n",
    "        Policy list in the stats array returned by the algorithm\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    steps: list of [action,state,reward]\n",
    "    \n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    state = start\n",
    "    \n",
    "    while state != end:\n",
    "        # get 1D index from state tuple\n",
    "        state_idx = state[0] * env.height + state[1]\n",
    "        action = policy[state_idx]\n",
    "        \n",
    "        # make a transition with choose=True\n",
    "        new_state, reward = env.transition(action, state, choose=True)\n",
    "        steps.append([action, list(state), reward])\n",
    "        state = new_state\n",
    "        \n",
    "    # append the goal state for visualization\n",
    "    steps.append(['G', list(end), 0])\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, start, stop, discount_factor=0.99, threshold=0.001):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs Value Iteartion algorithm on given environment\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        The GridWorld environment object\n",
    "        \n",
    "    start : (row, col)\n",
    "        The start state of the grid\n",
    "        \n",
    "    stop : (row, col)\n",
    "        The end state of the grid\n",
    "        \n",
    "    discount_factor : float\n",
    "        Factor that represents care for future rewards\n",
    "        \n",
    "    threshold: float\n",
    "        Value that represents cutoff for iterations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    policy: dict {(row, col): char}\n",
    "        The optimal policy dict with optimal action for each state\n",
    "        \n",
    "    V: dict {(row, col): float}\n",
    "        The optimal value dict with optimal values for each state\n",
    "        \n",
    "    stats: list of {policy:list,score:list}\n",
    "        The policy and V values for each iteration, used for visualization\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # improving the value function\n",
    "    def calculate_v(V, state, actions):\n",
    "        \n",
    "        \"\"\" \n",
    "        V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "        \n",
    "        \"\"\"\n",
    "        new_V = {'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
    "\n",
    "        for action in actions:\n",
    "            \n",
    "            # get all possible transitions list\n",
    "            possible_transitions = env.transition(action, state)\n",
    "            \n",
    "            for prob, next_state, reward in possible_transitions:\n",
    "                \n",
    "                new_V[action] += prob * \\\n",
    "                    (reward + discount_factor * V[next_state])\n",
    "\n",
    "        # key with max value\n",
    "        best_a = max(new_V, key=new_V.get)\n",
    "        \n",
    "        # max value in the dict\n",
    "        best_V = new_V[best_a]\n",
    "\n",
    "        return best_V, best_a\n",
    "\n",
    "    # getting all the states possible\n",
    "    all_states = set(env.rewards.keys())\n",
    "    \n",
    "    # Initialize V to 0\n",
    "    V = {}\n",
    "    for state in all_states:\n",
    "        V[state] = 0\n",
    "\n",
    "        \n",
    "    episode = 0\n",
    "    stats = []\n",
    "\n",
    "    # run until convergence\n",
    "    while True:\n",
    "\n",
    "        stats.append({})\n",
    "        \n",
    "        # delta is the improvement on the value function...\n",
    "        delta = 0\n",
    "\n",
    "        # iterating over all the states...\n",
    "        for state in all_states:\n",
    "            \n",
    "            # getting the best output from value function\n",
    "            best_V, _ = calculate_v(V, state, env.actions[state])\n",
    "            delta = max(delta, np.abs(best_V - V[state]))\n",
    "            V[state] = best_V\n",
    "\n",
    "        # collect stats for visuaization\n",
    "        policy_list, v_list = [], []\n",
    "        \n",
    "        # iterate on sorted states as policy and v are a list with index representing the states\n",
    "        for state in sorted(all_states):\n",
    "            \n",
    "            best_v, best_a = calculate_v(V, state, env.actions[state])\n",
    "            \n",
    "            policy_list.append(best_a)\n",
    "            \n",
    "            v_list.append(best_v)\n",
    "\n",
    "        # stats for this iteration\n",
    "        stats[episode] = {'policy': policy_list, 'score': v_list}\n",
    "\n",
    "        # we wanna calculate the number of episodes needed for convergence(optimal value function)\n",
    "        episode += 1\n",
    "\n",
    "        # check if converged\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    # get optimal policy\n",
    "    \n",
    "    policy = {}\n",
    "    \n",
    "    # we wanna calculate the best policy by putting in the best action for each state...(optimal policy)\n",
    "    for state in all_states:\n",
    "        _, best_a = calculate_v(V, state, env.actions[state])\n",
    "        policy[state] = best_a\n",
    "    \n",
    "    return policy, V, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im using the monte-carlo definition of Value Function... \n",
    "\n",
    "def policy_evaluation(env, policy, stop = (3,3), discount_factor = 0.99):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs policy evaluation on a given policy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        The GridWorld environment object\n",
    "        \n",
    "    start : (row, col)\n",
    "        The start state of the grid\n",
    "        \n",
    "    stop : (row, col)\n",
    "        The end state of the grid\n",
    "        \n",
    "    discount_factor : float\n",
    "        Factor that represents care for future rewards\n",
    "        \n",
    "    policy: dict\n",
    "        The action state relation that we wanna evaluate\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    V: dict(float)\n",
    "        The value for each state using the policy.\n",
    "    \"\"\"\n",
    "    # getting all the states(state space)\n",
    "    all_states = set(env.rewards.keys())\n",
    "    \n",
    "    # Initialize V to 0\n",
    "    V = {}\n",
    "    for state in all_states:\n",
    "        V[state] = 0\n",
    "        \n",
    "        \n",
    "    for state in sorted(all_states):\n",
    "        \n",
    "        # we create a dummy in order to preserve state after while\n",
    "        state_dummy = state\n",
    "        \n",
    "        # the exponent on the discount factor\n",
    "        i = 0\n",
    "        \n",
    "        while state_dummy != stop:\n",
    "            \n",
    "            # getting the reward for the next state\n",
    "            state_new, reward = env._new_state_reward(policy[state_dummy], state_dummy)\n",
    "            \n",
    "            # calculating the value function\n",
    "            V[state] += reward * (discount_factor ** i) \n",
    "            \n",
    "            # incrementing the state\n",
    "            state_dummy = state_new\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    # return the value function dict\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing out all the actions\n",
    "all_actions = ['U', 'D', 'L', 'R']\n",
    "\n",
    "# the greedy nature of policy iteration\n",
    "def _epsilon_greedy(action, epsilon):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns action according to epsilon greedy exploration scheme\n",
    "    \"\"\"\n",
    "    # getting a random array of 0.0 to 1.0\n",
    "    p = np.random.random()\n",
    "    \n",
    "    # condition for generating a random action\n",
    "    if p < (1 - epsilon):\n",
    "        return action\n",
    "    \n",
    "    else:\n",
    "        return np.random.choice(all_actions, 1)[0]\n",
    "\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes, epsilon, alpha, start = (0,0), stop = (3,3), discount_factor=0.99):\n",
    "    \n",
    "    \"\"\"Performs q learning algorithm on given environment\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        The GridWorld environment object\n",
    "        \n",
    "    num_episodes: int\n",
    "        The number of episodes to run\n",
    "        \n",
    "    epsilon: float\n",
    "        Epsilon value for exploration\n",
    "        \n",
    "    alpha: float\n",
    "        Learning rate\n",
    "        \n",
    "    start : (row, col)\n",
    "        The start state of the grid\n",
    "        \n",
    "    stop : (row, col)\n",
    "        The end state of the grid\n",
    "        \n",
    "    discount_factor : float\n",
    "        Factor that represents care for future rewards\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    policy: dict {(row, col): char}\n",
    "        The optimal policy dict with optimal action for each state\n",
    "        \n",
    "    V: dict {(row, col): float}\n",
    "        The optimal value dict with optimal values for each state\n",
    "        \n",
    "    stats: list of {policy:list,score:list}\n",
    "        The policy and V values for each iteration, used for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    # getting all possible states...\n",
    "    all_states = set(env.rewards.keys())\n",
    "\n",
    "    # q is the action value function...\n",
    "    # initialize Q[s][a]\n",
    "    Q = {}\n",
    "    \n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        \n",
    "        for action in all_actions:\n",
    "            Q[state][action] = 0\n",
    "            \n",
    "\n",
    "            \n",
    "    stats = []\n",
    "    \n",
    "    # iterating over the episodes\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        # starting from (0,0)\n",
    "        state = start\n",
    "        stats.append({})\n",
    "        \n",
    "        # until the stop state is reached\n",
    "        while state != stop:\n",
    "            \n",
    "            # taking best action for current state and use epsilon greedy...\n",
    "            action = max(Q[state], key=Q[state].get)\n",
    "            action = _epsilon_greedy(action, epsilon)\n",
    "\n",
    "            # transitioning to the next state and maximising the action..\n",
    "            new_state, reward = env.transition(action, state, choose=True)\n",
    "            best_next_action = max(Q[new_state], key=Q[new_state].get)\n",
    "\n",
    "            # calculating the temporal difference target\n",
    "            td_target = reward + discount_factor * \\\n",
    "                Q[new_state][best_next_action]\n",
    "            \n",
    "            # updating the Q-value\n",
    "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
    "\n",
    "            # going to the next state\n",
    "            state = new_state\n",
    "\n",
    "        # collecting stats\n",
    "        policy_list, q_list = [], []\n",
    "        \n",
    "        # iterate on sorted states as policy and v are a list with index representing the states\n",
    "        for state in sorted(all_states):\n",
    "            \n",
    "            # again, getting the best action\n",
    "            best_a = max(Q[state], key=Q[state].get)\n",
    "            \n",
    "            # getting the best q value for that state\n",
    "            best_q = Q[state][best_a]\n",
    "            \n",
    "            # putting the best action into the policy list(optimal policy)\n",
    "            if state != stop:\n",
    "                policy_list.append(best_a)\n",
    "                \n",
    "            # goal state reached!\n",
    "            else:\n",
    "                policy_list.append('G')\n",
    "                \n",
    "             # the best q value is appended   \n",
    "            q_list.append(best_q)\n",
    "\n",
    "        # add steps according to current policy to visualize agent's actions\n",
    "        stats[episode] = {\n",
    "            'policy': policy_list,\n",
    "            'score': q_list,\n",
    "            'steps': play_game(\n",
    "                env,\n",
    "                start,\n",
    "                stop,\n",
    "                policy_list)}\n",
    "\n",
    "    # optimal policy and Value function\n",
    "    policy, V, = {}, {}\n",
    "    \n",
    "    # iterating over all states\n",
    "    for state in all_states:\n",
    "        \n",
    "        # getting the best action\n",
    "        best_a = max(Q[state], key=Q[state].get)\n",
    "        \n",
    "        # again storing the best q value\n",
    "        best_q = Q[state][best_a]\n",
    "        \n",
    "        # putting the best action into the optimal policy\n",
    "        policy[state] = best_a if state != stop else 'G'\n",
    "        \n",
    "        # putting in the best action-value function\n",
    "        V[state] = best_q\n",
    "\n",
    "    return policy, V, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Agent/Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the grid object...\n",
    "env = grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying value iteration and getting the return objects...\n",
    "policy_val_iter, V_val_iter, stats_val_iter = value_iteration(env,start=(1,0),stop=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R' 'R' 'R' 'D']\n",
      " ['U' 'U' 'R' 'D']\n",
      " ['D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'U']]\n"
     ]
    }
   ],
   "source": [
    "# printing out the optimal policy obtained from value iteration\n",
    "print_grid(env, policy_val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86.36413640238297 88.96553043345659 91.58232574832843 93.69969985383423]\n",
      " [84.37867721159373 87.1196669488651 93.99527972321908 96.4100771203664]\n",
      " [76.335435930807 92.97504454302948 96.68547553197288 99.19185503857933]\n",
      " [85.1940856411934 88.63980510086662 99.19185503857932 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the optimal value function obtained from value iteration\n",
    "print_grid(env, V_val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying policy evaluation on the policy obtained from value iteration\n",
    "value_function = policy_evaluation(env, policy_val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.19800998 92.119202 94.0598 96.02]\n",
      " [88.2960298802 90.19800998 96.02 98.0]\n",
      " [94.0598 96.02 98.0 100.0]\n",
      " [96.02 98.0 100.0 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the value function obtained\n",
    "print_grid(env, value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying q-learning and obtaining the optimal policy and value function...\n",
    "num_episodes = 500\n",
    "epsilon = 0.05\n",
    "alpha = 0.5\n",
    "\n",
    "policy_q_learn, V_q_learn, stats_q_learn = q_learning(env, num_episodes, epsilon, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R' 'R' 'R' 'D']\n",
      " ['R' 'U' 'D' 'D']\n",
      " ['L' 'L' 'R' 'D']\n",
      " ['R' 'D' 'R' 'G']]\n"
     ]
    }
   ],
   "source": [
    "# printing out the optimal policy obtained from policy iteration\n",
    "print_grid(env, policy_q_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83.16472463096417 87.53554182030925 90.54961017031259 92.44684233253933]\n",
      " [80.02526838810203 85.124420271248 90.42659757010276 90.64016647273385]\n",
      " [65.90783251886765 13.131151083349256 96.31536439872912\n",
      "  95.28971566668275]\n",
      " [81.0674489698741 92.04150327557775 99.45606707874634 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the optimal value function obtained from policy iteration\n",
    "print_grid(env, V_q_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying policy evaluation on the policy obtained from q learning\n",
    "value_function = policy_evaluation(env, policy_q_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
