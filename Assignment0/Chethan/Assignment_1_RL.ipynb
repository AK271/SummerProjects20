{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GridWorld Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "\n",
    "# setting the environment using the GridWorld Class\n",
    "class GridWorld:\n",
    "    \n",
    "    \"\"\"\n",
    "    Grid environment with following stochastic property:\n",
    "    | Agent Action | Possible Actions  |  Probability  |\n",
    "    | :----------: | :---------------: | :-----------: |\n",
    "    |      UP      |  UP, RIGHT, LEFT  | 0.8, 0.1, 0.1 |\n",
    "    |     DOWN     | DOWN, RIGHT, LEFT | 0.8, 0.1, 0.1 |\n",
    "    |     LEFT     |  LEFT, UP, DOWN   | 0.8, 0.1, 0.1 |\n",
    "    |    RIGHT     |  RIGHT, UP, DOWN  | 0.8, 0.1, 0.1 |\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # listing out all the possible actions...\n",
    "    POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']\n",
    "\n",
    "    \n",
    "    def __init__(self, size, rewards, actions):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the GridWorld object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size : tuple (row,col)\n",
    "        \n",
    "        rewards: dict {(row,col):int}\n",
    "            A dictionary with reward values for each state in the grid\n",
    "            \n",
    "        actions: dict ({row,col}:list)\n",
    "            A dictionary that associates all possible actions for each state\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.height, self.width = size\n",
    "\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "        self.num_states = np.prod(size)\n",
    "        self.num_actions = len(GridWorld.POSSIBLE_ACTIONS)\n",
    "\n",
    "        \n",
    "    \n",
    "    def _limit_coordinates(self, state):\n",
    "        \n",
    "        \"\"\"\n",
    "        Limits the coordinates if/after collision with grid wall\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple (row, col)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state: tuple(row, col)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # pretty straightforward...\n",
    "        i, j = state\n",
    "        \n",
    "        if i < 0:\n",
    "            i = 0\n",
    "            \n",
    "        elif i > self.height - 1:\n",
    "            i = self.height - 1\n",
    "            \n",
    "        if j < 0:\n",
    "            j = 0\n",
    "            \n",
    "        elif j > self.width - 1:\n",
    "            j = self.width - 1\n",
    "            \n",
    "        return (i, j)\n",
    "\n",
    "    \n",
    "    def _new_state_reward(self, action, state):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the coordinates of a resultant state and its rewards\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action: char\n",
    "            The character representing the action taken\n",
    "            \n",
    "        state: tuple (row, col)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state: tuple (row, col)\n",
    "            The new state reached by taking the action\n",
    "            \n",
    "        reward: int\n",
    "            The reward for the state\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # again, we are updating the agent's location...\n",
    "        i, j = state\n",
    "        \n",
    "        if action == 'U':\n",
    "            i, j = i - 1, j\n",
    "            \n",
    "        elif action == 'D':\n",
    "            i, j = i + 1, j\n",
    "            \n",
    "        elif action == 'R':\n",
    "            i, j = i, j + 1\n",
    "            \n",
    "        elif action == 'L':\n",
    "            i, j = i, j - 1\n",
    "\n",
    "        # make sure the new state is not out of grid\n",
    "        new_state = self._limit_coordinates((i, j))\n",
    "\n",
    "        \n",
    "        return new_state, self.rewards.get(new_state)\n",
    "    \n",
    "\n",
    "    def transition(self, action, state, choose=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        The stochastic transition model of the grid\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : char\n",
    "            The character representing the action taken\n",
    "            \n",
    "        state : tuple (row, col)\n",
    "            The current state from where transition is occuring\n",
    "            \n",
    "        choose : boolean\n",
    "            If True: environment takes stochastic action and returns resultant\n",
    "                     state and reward\n",
    "                     \n",
    "            If False: environment returns a list of all possible actions with\n",
    "                      corresponding reward and probabilties\n",
    "                      \n",
    "        Returns\n",
    "        -------\n",
    "        If choose == True\n",
    "        \n",
    "            state : tuple (row, col)\n",
    "            \n",
    "            reward: int\n",
    "            \n",
    "        If choose == Flase\n",
    "            A list with following tuple:\n",
    "            \n",
    "                prob: float\n",
    "                    Probabilty with which environment selects the transition\n",
    "                    \n",
    "                state: tuple (row, col)\n",
    "                \n",
    "                reward: int\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # as the blueprint above suggests, we create the function body...\n",
    "        def stochastic_transition(possible_actions, prob):\n",
    "            \n",
    "            if not choose:\n",
    "                # create and return a list of all possible actions\n",
    "                result = []\n",
    "                \n",
    "                for i, a in enumerate(possible_actions):\n",
    "                    coord, reward = self._new_state_reward(a, state)\n",
    "                    result.append((prob[i], coord, reward))\n",
    "                    \n",
    "                return result\n",
    "            \n",
    "            else:\n",
    "                # choose a random action with given probabilities\n",
    "                a = np.random.choice(possible_actions, 1, p = prob)\n",
    "                coord, reward = self._new_state_reward(a, state)\n",
    "                \n",
    "                return coord, reward\n",
    "\n",
    "        \n",
    "        # we already have the state transition probabilities, they are initialised in the next function\n",
    "        # going up the grif\n",
    "        if action == 'U':\n",
    "            return stochastic_transition(['U', 'R', 'L'], [0.8, 0.1, 0.1])\n",
    "        \n",
    "        # going down the grid\n",
    "        elif action == 'D':\n",
    "            return stochastic_transition(['D', 'R', 'L'], [0.8, 0.1, 0.1])\n",
    "        \n",
    "        # going right\n",
    "        elif action == 'R':\n",
    "            return stochastic_transition(['R', 'U', 'D'], [0.8, 0.1, 0.1])\n",
    "        \n",
    "        # going left\n",
    "        elif action == 'L':\n",
    "            return stochastic_transition(['L', 'U', 'D'], [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "        \n",
    "def grid():\n",
    "    \"\"\"Utility function, returns 4x4 GridWorld object with rewards and actions\n",
    "    \"\"\"\n",
    "\n",
    "    # dict with rewards for states of the grid\n",
    "    rewards = {\n",
    "        (0, 0): -1, (0, 1): -1, (0, 2): -1, (0, 3): -1,\n",
    "        (1, 0): -1, (1, 1): -1, (1, 2): -1, (1, 3): -1,  # start state is 1x0\n",
    "        (2, 0): -1, (2, 1): -70, (2, 2): -1, (2, 3): -1,  # bad state is 2x1\n",
    "        (3, 0): -1, (3, 1): -1, (3, 2): -1, (3, 3): 100  # goal state is 3x3\n",
    "    }\n",
    "\n",
    "    # dict with actions allowed for the grid states\n",
    "    actions = {\n",
    "        (0, 0): ['R', 'D'], (0, 1): ['R', 'L', 'D'],\n",
    "        (0, 2): ['R', 'L', 'D'], (0, 3): ['L', 'D'],\n",
    "        (1, 0): ['R', 'U', 'D'], (1, 1): ['R', 'L', 'U', 'D'],\n",
    "        (1, 2): ['R', 'L', 'U', 'D'], (1, 3): ['L', 'U', 'D'],\n",
    "        (2, 0): ['R', 'U', 'D'], (2, 1): ['R', 'L', 'U', 'D'],\n",
    "        (2, 2): ['R', 'L', 'U', 'D'], (2, 3): ['L', 'U', 'D'],\n",
    "        (3, 0): ['R', 'U'], (3, 1): ['R', 'L', 'U'],\n",
    "        (3, 2): ['R', 'L', 'U'], (3, 3): []\n",
    "    }\n",
    "\n",
    "    return GridWorld(size=(4, 4), rewards=rewards, actions=actions)\n",
    "\n",
    "\n",
    "def print_grid(env, content_dict):\n",
    "    \n",
    "    \"\"\" \n",
    "    Utility function that prints the grid environment with given content\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    env: GridWorld\n",
    "    \n",
    "    content_dict: dict {(row,col):object}\n",
    "    \n",
    "    \"\"\"\n",
    "    # getting even coordinates in the grid..\n",
    "    grid = np.arange(env.num_states, dtype=object).reshape(env.height, env.width)\n",
    "    \n",
    "    # content_dict maps the coordinate to the content(policy, value function, etc)...\n",
    "    for coord, content in content_dict.items():\n",
    "        grid[coord[0], coord[1]] = content\n",
    "    \n",
    "    # printing out the np matrix\n",
    "    print(grid)\n",
    "\n",
    "\n",
    "def play_game(env, start, end, policy):\n",
    "    \n",
    "    \"\"\"\n",
    "    Utility function that follows given policy from start to end and\n",
    "    returns a list of action, state and reward for each step\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "    \n",
    "    start : (row, col)\n",
    "    \n",
    "    end : (row, col)\n",
    "    \n",
    "    policy : list of optimal actions for each state\n",
    "        Policy list in the stats array returned by the algorithm\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    steps: list of [action,state,reward]\n",
    "    \n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    state = start\n",
    "    \n",
    "    while state != end:\n",
    "        # get 1D index from state tuple\n",
    "        state_idx = state[0] * env.height + state[1]\n",
    "        action = policy[state_idx]\n",
    "        \n",
    "        # make a transition with choose=True\n",
    "        new_state, reward = env.transition(action, state, choose=True)\n",
    "        steps.append([action, list(state), reward])\n",
    "        state = new_state\n",
    "        \n",
    "    # append the goal state for visualization\n",
    "    steps.append(['G', list(end), 0])\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, start, stop, discount_factor=0.99, threshold=0.001):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs Value Iteartion algorithm on given environment\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        The GridWorld environment object\n",
    "        \n",
    "    start : (row, col)\n",
    "        The start state of the grid\n",
    "        \n",
    "    stop : (row, col)\n",
    "        The end state of the grid\n",
    "        \n",
    "    discount_factor : float\n",
    "        Factor that represents care for future rewards\n",
    "        \n",
    "    threshold: float\n",
    "        Value that represents cutoff for iterations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    policy: dict {(row, col): char}\n",
    "        The optimal policy dict with optimal action for each state\n",
    "        \n",
    "    V: dict {(row, col): float}\n",
    "        The optimal value dict with optimal values for each state\n",
    "        \n",
    "    stats: list of {policy:list,score:list}\n",
    "        The policy and V values for each iteration, used for visualization\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # improving the value function\n",
    "    def calculate_v(V, state, actions):\n",
    "        \n",
    "        \"\"\" \n",
    "        V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "        \n",
    "        \"\"\"\n",
    "        new_V = {'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
    "\n",
    "        for action in actions:\n",
    "            \n",
    "            # get all possible transitions list\n",
    "            possible_transitions = env.transition(action, state)\n",
    "            \n",
    "            for prob, next_state, reward in possible_transitions:\n",
    "                \n",
    "                new_V[action] += prob * \\\n",
    "                    (reward + discount_factor * V[next_state])\n",
    "\n",
    "        # key with max value\n",
    "        best_a = max(new_V, key=new_V.get)\n",
    "        \n",
    "        # max value in the dict\n",
    "        best_V = new_V[best_a]\n",
    "\n",
    "        return best_V, best_a\n",
    "\n",
    "    # getting all the states possible\n",
    "    all_states = set(env.rewards.keys())\n",
    "    \n",
    "    # Initialize V to 0\n",
    "    V = {}\n",
    "    for state in all_states:\n",
    "        V[state] = 0\n",
    "\n",
    "        \n",
    "    episode = 0\n",
    "    stats = []\n",
    "\n",
    "    # run until convergence\n",
    "    while True:\n",
    "\n",
    "        stats.append({})\n",
    "        \n",
    "        # delta is the improvement on the value function...\n",
    "        delta = 0\n",
    "\n",
    "        # iterating over all the states...\n",
    "        for state in all_states:\n",
    "            \n",
    "            # getting the best output from value function\n",
    "            best_V, _ = calculate_v(V, state, env.actions[state])\n",
    "            delta = max(delta, abs(best_V - V[state]))\n",
    "            V[state] = best_V\n",
    "\n",
    "        # collect stats for visuaization\n",
    "        policy_list, v_list = [], []\n",
    "        \n",
    "        # iterate on sorted states as policy and v are a list with index representing the states\n",
    "        for state in sorted(all_states):\n",
    "            \n",
    "            best_v, best_a = calculate_v(V, state, env.actions[state])\n",
    "            \n",
    "            policy_list.append(best_a)\n",
    "            \n",
    "            v_list.append(best_v)\n",
    "\n",
    "        # stats for this iteration\n",
    "        stats[episode] = {'policy': policy_list, 'score': v_list}\n",
    "\n",
    "        # we wanna calculate the number of episodes needed for convergence(optimal value function)\n",
    "        episode += 1\n",
    "\n",
    "        # check if converged\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    # get optimal policy\n",
    "    \n",
    "    policy = {}\n",
    "    \n",
    "    # we wanna calculate the best policy by putting in the best action for each state...(optimal policy)\n",
    "    for state in all_states:\n",
    "        _, best_a = calculate_v(V, state, env.actions[state])\n",
    "        policy[state] = best_a\n",
    "    \n",
    "    return policy, V, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im using the monte-carlo definition of Value Function... \n",
    "\n",
    "def policy_evaluation(env, policy, threshold = 0.001, discount_factor = 0.99):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs policy evaluation on a given policy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        The GridWorld environment object\n",
    "        \n",
    "    start : (row, col)\n",
    "        The start state of the grid\n",
    "        \n",
    "    stop : (row, col)\n",
    "        The end state of the grid\n",
    "        \n",
    "    discount_factor : float\n",
    "        Factor that represents care for future rewards\n",
    "        \n",
    "    policy: dict\n",
    "        The action state relation that we wanna evaluate\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    V: dict(float)\n",
    "        The value for each state using the policy.\n",
    "    \"\"\"\n",
    "    # getting all the states(state space)\n",
    "    all_states = set(env.rewards.keys())\n",
    "    \n",
    "    # assigning V(s) all to 0...\n",
    "    V = {}\n",
    "    for state in all_states:\n",
    "        V[state] = 0\n",
    "   \n",
    "    while(True):\n",
    "        \n",
    "        # assigning delta = 0\n",
    "        delta = 0\n",
    "        \n",
    "        # primary loop \n",
    "        for state in all_states:\n",
    "            \n",
    "            # assigning the value function to a dummy variable\n",
    "            dummy_v = V[state]\n",
    "            \n",
    "            sum_v = 0\n",
    "            \n",
    "            # get all possible transitions list\n",
    "            possible_transitions = env.transition(policy[state], state)\n",
    "            \n",
    "            for prob, next_state, reward in possible_transitions:\n",
    "                \n",
    "                # updating V[state]\n",
    "                sum_v = sum_v + (prob * (reward + discount_factor * V[next_state]))\n",
    "            \n",
    "            # making updates\n",
    "            V[state] = sum_v\n",
    "            \n",
    "            # updating delta\n",
    "            delta = max(delta, abs(dummy_v - V[state]))\n",
    "        \n",
    "        # condition to end while loop\n",
    "        if delta < threshold:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # return the value function dict\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1\n",
    "# initialisation of the value function, policy for all in state space...\n",
    "def initialisation():\n",
    "    \n",
    "    # initialising the value function dictionaries\n",
    "    all_states = set(env.rewards.keys())\n",
    "    V = {}\n",
    "    for state in all_states:\n",
    "        V[state] = 0\n",
    "       \n",
    "    # policy initiation randomly\n",
    "    policy = {\n",
    "        (0, 0): \"R\", (0, 1): \"L\", (0, 2): \"U\", (0, 3): \"D\",\n",
    "        (1, 0): \"U\", (1, 1): \"L\", (1, 2): \"D\", (1, 3): \"U\",  # start state is 1x0\n",
    "        (2, 0): \"L\", (2, 1): \"U\", (2, 2): \"R\", (2, 3): \"R\",  # bad state is 2x1\n",
    "        (3, 0): \"D\", (3, 1): \"R\", (3, 2): \"D\", (3, 3): \"L\" # goal state is 3x3\n",
    "    }\n",
    "    \n",
    "    # returning the value functions and policies\n",
    "    return V, policy\n",
    " \n",
    "# step 2\n",
    "# policy evaluation is already created above, so step 2 is taken care of...\n",
    "\n",
    "\n",
    "# step 3\n",
    "# performing policy iteration on the random policy...\n",
    "def policy_iteration(env, discount_factor = 0.99):\n",
    "    \n",
    "    # policy stable is our control variable\n",
    "    \n",
    "    V, policy = initialisation()\n",
    "    \n",
    "    all_states = set(env.rewards.keys())\n",
    "  \n",
    "    while(True):\n",
    "        \n",
    "        policy_unstable = False\n",
    "        \n",
    "        # we wanna call in policy evaluation...\n",
    "        V = policy_evaluation(env, policy)\n",
    "    \n",
    "        # iterating over the states\n",
    "        for state in all_states:\n",
    "        \n",
    "            # we take the action according to the policy\n",
    "            action_pol = policy[state]\n",
    "        \n",
    "            # we wanna see as to which action would lead maximum value function\n",
    "            check_val = {\"U\" : 0, \"L\" : 0, \"D\" : 0, \"R\" : 0}\n",
    "        \n",
    "            # we used the policy, but we wanna see if the action in the policy was optimal or not....\n",
    "            for action in env.actions[state]:\n",
    "            \n",
    "                # getting the possible transitions to evaluate the value function...\n",
    "                possible_transitions = env.transition(action, state)\n",
    "            \n",
    "                for prob, next_state, reward in possible_transitions:\n",
    "                \n",
    "                    check_val[action] += prob * (reward + (discount_factor * V[next_state]))\n",
    "                \n",
    "            # now we wanna get the best action according to the value function...\n",
    "            best_action = max(check_val, key = check_val.get)\n",
    "            policy[state] = best_action\n",
    "        \n",
    "            # improving the policy\n",
    "            if action_pol != policy[state]:\n",
    "                policy_unstable = True\n",
    "    \n",
    "        # checking policy_unstable and deciding if we wanna continue or not\n",
    "        if policy_unstable:\n",
    "                    continue\n",
    "        \n",
    "        # if the policy is stable we return the policy and value function\n",
    "        if (policy_unstable == False):\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Agent/Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the grid object...\n",
    "env = grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying value iteration and getting the return objects...\n",
    "policy_val_iter, V_val_iter, stats_val_iter = value_iteration(env,start=(1,0),stop=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R' 'R' 'R' 'D']\n",
      " ['U' 'U' 'R' 'D']\n",
      " ['D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'U']]\n"
     ]
    }
   ],
   "source": [
    "# printing out the optimal policy obtained from value iteration\n",
    "print_grid(env, policy_val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86.36413640238297 88.96553043345659 91.58232574832843 93.69969985383423]\n",
      " [84.37867721159373 87.1196669488651 93.99527972321908 96.4100771203664]\n",
      " [76.335435930807 92.97504454302948 96.68547553197288 99.19185503857933]\n",
      " [85.1940856411934 88.63980510086662 99.19185503857932 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the optimal value function obtained from value iteration\n",
    "print_grid(env, V_val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying policy evaluation on the policy obtained from value iteration\n",
    "value_function = policy_evaluation(env, policy_val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4087.580135990868 4146.030358613857 4204.827456706381 4252.405091732424]\n",
      " [4042.9682057710156 4104.555375544737 4259.046218537807\n",
      "  4313.307027759986]\n",
      " [4188.406436988312 4252.579871559484 4319.496032144209 4375.814301169395]\n",
      " [4243.243547458407 4304.93834962499 4375.813434169742 4337.3469326859085]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the value function obtained\n",
    "print_grid(env, value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pol_iter, V_pol_iter = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'D' 'D']\n",
      " ['D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'U']]\n"
     ]
    }
   ],
   "source": [
    "print_grid(env, policy_pol_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4113.763845258498 4162.2470575728685 4210.74690582428 4255.386772943408]\n",
      " [4151.532790607644 4204.787995023508 4265.658147846352 4315.959135399683]\n",
      " [4192.707857256903 4264.8917497927605 4322.102454042532\n",
      "  4378.004919699393]\n",
      " [4246.597971593204 4308.21675575744 4378.004050543675 4339.513235520164]]\n"
     ]
    }
   ],
   "source": [
    "print_grid(env, V_pol_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
