{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ValueIteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H-C3WW2ng6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W4G8EW9nnQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a grid environment\n",
        "def gridEnv(curr_state,action):\n",
        "  # In the 4*4 grid world state 0 and state 15 are terminal states\n",
        "  if(curr_state==0 or curr_state==15):\n",
        "    next_state=curr_state\n",
        "    reward=0\n",
        "  else:\n",
        "    #action 0 --> UP, action 1 --> DOWN, action 2 --> RIGHT, action 3 --> LEFT\n",
        "    #if the action results in the agent coming out of the grid then the curr_state will not change\n",
        "    reward=-1\n",
        "    if(action==0):\n",
        "      next_state=curr_state-4\n",
        "      if(next_state<0):\n",
        "        next_state=curr_state\n",
        "    elif(action==1):\n",
        "      next_state=curr_state+4\n",
        "      if(next_state>=16):\n",
        "        next_state=curr_state\n",
        "    elif(action==2):\n",
        "      if((curr_state+1)%4==0):\n",
        "        next_state=curr_state\n",
        "      else:\n",
        "        next_state=curr_state+1\n",
        "    elif(action==3):\n",
        "      if(curr_state%4==0):\n",
        "        next_state=curr_state\n",
        "      else:\n",
        "        next_state=curr_state-1\n",
        "#if the next_state is the terminanl state the reward is 0 otherwise -1\n",
        "  #In this grid world the state transition probability for the given action and state is 1.\n",
        "  items=[1,next_state,reward]\n",
        "\n",
        "  return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPC8frzYnvbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valueIteration(policy,gamma=0.999,theta=0.0000001):\n",
        "    #Initializing the value fucntion as zero\n",
        "    vf=np.zeros(16)\n",
        "    while(True):\n",
        "        delta=0\n",
        "        #Iterating over all the states\n",
        "        for curr_state in range(16):\n",
        "            avp=np.zeros(4)\n",
        "            for curr_action in range(4):\n",
        "                stateTprob, next_state, reward = gridEnv(curr_state, curr_action)\n",
        "                avp[curr_action] = stateTprob*(reward+gamma*vf[next_state])\n",
        "            b_action=np.argmax(avp)\n",
        "            #taking the maximum value function \n",
        "            b_vf=np.max(avp)\n",
        "            #Updating the policy\n",
        "            policy[curr_state]=np.eye(4)[b_action]\n",
        "            delta=max(delta,np.abs(vf[curr_state]-b_vf))\n",
        "            #Assiging the state its new value function\n",
        "            vf[curr_state]=b_vf\n",
        "        #breaking the loop if delta is less than theta\n",
        "        if(delta<theta):\n",
        "            break\n",
        "    return policy,vf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yAvxDFIn2Sr",
        "colab_type": "code",
        "outputId": "7bf73ccf-d890-4a51-a4e2-d7418c05080f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#initializing random policy where each action has equal probability\n",
        "policy = np.ones([16, 4])/4\n",
        "policy,vf= valueIteration(policy)\n",
        "print(f'Optimal Policy:\\n\\n {policy}\\n')\n",
        "print(f'Optimal Value function:\\n\\n {vf.reshape(4, 4)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Policy:\n",
            "\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "\n",
            "Optimal Value function:\n",
            "\n",
            " [[ 0.       -1.       -1.999    -2.997001]\n",
            " [-1.       -1.999    -2.997001 -1.999   ]\n",
            " [-1.999    -2.997001 -1.999    -1.      ]\n",
            " [-2.997001 -1.999    -1.        0.      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}