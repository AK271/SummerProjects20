{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolicyIteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD8e7i8Mfmh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_O8zBTQf-_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a grid environment\n",
        "def gridEnv(curr_state,action):\n",
        "  # In the 4*4 grid world state 0 and state 15 are terminal states\n",
        "  if(curr_state==0 or curr_state==15):\n",
        "    next_state=curr_state\n",
        "    reward=0\n",
        "  else:\n",
        "    #action 0 --> UP, action 1 --> DOWN, action 2 --> RIGHT, action 3 --> LEFT\n",
        "    #if the action results in the agent coming out of the grid then the curr_state will not change\n",
        "    reward=-1\n",
        "    if(action==0):\n",
        "      next_state=curr_state-4\n",
        "      if(next_state<0):\n",
        "        next_state=curr_state\n",
        "    elif(action==1):\n",
        "      next_state=curr_state+4\n",
        "      if(next_state>=16):\n",
        "        next_state=curr_state\n",
        "    elif(action==2):\n",
        "      if((curr_state+1)%4==0):\n",
        "        next_state=curr_state\n",
        "      else:\n",
        "        next_state=curr_state+1\n",
        "    elif(action==3):\n",
        "      if(curr_state%4==0):\n",
        "        next_state=curr_state\n",
        "      else:\n",
        "        next_state=curr_state-1\n",
        "#if the next_state is the terminanl state the reward is 0 otherwise -1\n",
        "  #In this grid world the state transition probability for the given action and state is 1.\n",
        "  items=[1,next_state,reward]\n",
        "\n",
        "  return items\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TdLhISZgRfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def polyEval(policy,gamma=1,theta=0.00000001):\n",
        "  #Initializing the value function to zero\n",
        "  vf=np.zeros(16)\n",
        "  while True:\n",
        "    delta=0\n",
        "    #Iterating over the states\n",
        "    for curr_state in range(16):\n",
        "      new_vf=0\n",
        "      #iterating over the policy of a given state\n",
        "      for curr_action, p_action in enumerate(policy[curr_state]):\n",
        "        stateTprob,next_state,reward=gridEnv(curr_state,curr_action)\n",
        "        new_vf+=p_action*stateTprob*(reward+gamma*vf[next_state])\n",
        "      #new_vf is calcuated using Bellman-equation\n",
        "      #delta quantity is used to track the difference between initial VF and updated VF\n",
        "      delta=max(delta,np.abs(new_vf-vf[curr_state]))\n",
        "      vf[curr_state]=new_vf\n",
        "    if(delta<theta):\n",
        "      break\n",
        "  #updated value function is returned\n",
        "  return vf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XViwwwGeg8ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def polyIteration(policy, gamma=0.9999):\n",
        "    #Initilizing the current policy as opitmal policy\n",
        "    policy_stable = True\n",
        "    #Obtaining the value function from the current policy\n",
        "    vf = polyEval(policy)\n",
        "    #Iterating over all the states\n",
        "    for curr_state in range(16):\n",
        "        #b_action1 stores the optimal action for the current state as per the policy\n",
        "        b_action1 = np.argmax(policy[curr_state])\n",
        "        avp = np.zeros(4)\n",
        "        #iterating over all the actions\n",
        "        for curr_action in range(4):\n",
        "            stateTprob, next_state, reward = gridEnv(curr_state, curr_action)\n",
        "            avp[curr_action] = stateTprob*(reward+gamma*vf[next_state])\n",
        "        #Bellman Optimality equation, taken greedily\n",
        "        b_action2 = np.argmax(avp)\n",
        "        #Updating the policy\n",
        "        policy[curr_state] = np.eye(4)[b_action2]\n",
        "        #If the updated best action is not same as the initial best action for a given state, the policy is still not optimal\n",
        "        if(b_action1 != b_action2):\n",
        "            policy_stable = False\n",
        "    if(policy_stable):\n",
        "        return policy,vf\n",
        "    else:\n",
        "        return polyIteration(policy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIXZNFstgiNF",
        "colab_type": "code",
        "outputId": "036a35ac-a5d0-4cf4-d881-c0fb9eba262c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#initializing random policy where each action has equal probability\n",
        "policy = np.ones([16, 4])/4\n",
        "policy,vf= polyIteration(policy)\n",
        "print(f'Optimal Policy:\\n\\n{policy}\\n')\n",
        "print(f'Optimal Value function:\\n\\n {vf.reshape(4, 4)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Policy:\n",
            "\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "\n",
            "Optimal Value function:\n",
            "\n",
            " [[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}